name: 6 - Databricks Streaming Job
# Run this workflow on demand
on:
  push:
    branches:
      - main
    paths:
      - .github/workflows/build-publish-wheel-file.yml
  workflow_dispatch:

env:
  ORGANISATION_NAME: endk
  PROJECT_NAME: aggregations
  WHEEL_STORAGE_NAME: 'aggregationwheels'
  WHEEL_CONTAINER_NAME: 'wheels'

jobs:
  # Set the job key. The key is displayed as the job name
  # when a job name is not provided
  databricks_streaming_job_deploy:
    # Name the Job
    name: Create Databricks Streaming Job Infrastructure
    # Set the type of machine to run on
    # Resource Group set for automated trigger purposes, if ran manually it is overriden
    # All ENV Vars set bellow are needed for all the actions to run successfuly
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [
          {
            long: rg-DataHub-Aggregations-U,
            short: u,
            name: Development
          },
          {
            long: rg-DataHub-Aggregations-T,
            short: t,
            name: Test
          },
          {
            long: rg-DataHub-Aggregations-B,
            short: b,
            name: Preprod
          },
          {
            long: rg-DataHub-Aggregations-P,
            short: p,
            name: Production
          }
        ]
    environment:
      name: ${{ matrix.environment.long }}

    steps:
      # https://github.com/actions/checkout
      - name: Checkout code
        uses: actions/checkout@master
        
      - name: Set Environment Secrets
        run: |  
          echo "ARM_TENANT_ID=${{ secrets.TENANT_ID }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_ID=${{ secrets.SPN_ID }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_OBJECT_ID=${{ secrets.SPN_OBJECT_ID }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_SECRET=${{ secrets.SPN_SECRET }}" >> $GITHUB_ENV
          echo "ARM_SUBSCRIPTION_ID=${{ secrets.SUBSCRIPTION_ID }}" >> $GITHUB_ENV
          
                
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1.2.1
        with:
          terraform_wrapper: false
      # https://github.com/actions/setup-python
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8.6' # Version range or exact version of a Python version to use, using SemVer's version range syntax
          architecture: 'x64' # optional x64 or x86. Defaults to x64 if not specified

      - name: Azure CLI Install and Login
        uses: ./.github/actions/azure-cli-install-login

      - name: Check If Wheel Repository Storage exists
        id: wheel-storage-exists
        run: |
          storage_exists=$(az storage account check-name --name ${{ env.WHEEL_STORAGE_NAME }}${{ matrix.environment.short }} | python3 -c "import sys, json; print(not json.load(sys.stdin)['nameAvailable'])")
          echo "::set-output name=wheel-storage-exists::${storage_exists}"

      - name: Create Wheel Repository Storage if needed
        run: |
          az storage account create --resource-group ${{ matrix.environment.long }} --name ${{ env.WHEEL_STORAGE_NAME }}${{ matrix.environment.short }} --sku Standard_LRS --encryption-services blob
          account_key=$(az storage account keys list --resource-group ${{ matrix.environment.long }} --account-name ${{ env.WHEEL_STORAGE_NAME }}${{ matrix.environment.short }} --query '[0].value' -o tsv)
          az storage container create --name ${{ env.WHEEL_CONTAINER_NAME }} --account-name ${{ env.WHEEL_STORAGE_NAME }}${{ matrix.environment.short }} --account-key $account_key --public-access blob
        if: steps.wheel-storage-exists.outputs.wheel-storage-exists == 'False'

      - name: Create Python Wheel for Databricks Jobs
        working-directory: ./source/databricks
        run: |
          echo "1.0" > VERSION
          pip install wheel
          python setup.py sdist bdist_wheel

      - name: Upload Wheel
        run: |
          version="1.0"
          account_key=$(az storage account keys list --resource-group ${{ matrix.environment.long }} --account-name ${{ env.WHEEL_STORAGE_NAME }}${{ matrix.environment.short }} --query '[0].value' -o tsv)
          az storage blob upload --account-name ${{ env.WHEEL_STORAGE_NAME }}${{ matrix.environment.short }} --container-name ${{ env.WHEEL_CONTAINER_NAME }} \
          --name "geh_stream-${version}-py3-none-any.whl" \
          --file "./source/databricks/dist/geh_stream-${version}-py3-none-any.whl" \
          --account-key $account_key

      - name: Obtain Databricks Workspace ID and Url
        id: obtain-db-id-url
        uses: ./.github/actions/obtain-databricks-id-url
        
      - name: Databricks CLI Install,token generation and Connect
        uses: ./.github/actions/databricks-cli-install-connect
        with:
          workspace-url: ${{ steps.obtain-db-id-url.outputs.workspace-url }}
      
      # Download wheel file - step 3 of 4
      - name: Construct Wheel File Name
        uses: ./.github/actions/construct-wheel-file-name
        with:
          wheel_version: "1.0"

      # Download wheel file - step 4 of 4
      # https://github.com/suisei-cn/actions-download-file
      - uses: suisei-cn/actions-download-file@v1
        name: Download the Wheel File
        with:
          url: "https://aggregationwheels${{ matrix.environment.short }}.blob.core.windows.net/wheels/${{ env.WHEEL_FILE_NAME }}"
          target: wheels/

      - name: Copy Wheel File to Databricks Workspace
        uses: ./.github/actions/copy-wheel-file-to-databricks

      - name: Copy Job Definitions to DBFS
        run: |    
          dbfs cp --overwrite -r ./source/databricks/streaming-jobs "dbfs:/streaming"
      
      # Obtain keys from keyvault and store them in {steps.myGetSecretAction.outputs}
      - name: Azure key vault - Get Secrets
        uses: Azure/get-keyvault-secrets@v1.2
        with:
          keyvault: "kvaggregationsendk${{ matrix.environment.short }}"
          secrets: 'aggregation-evh-listening-key'  # comma separated list of secret keys that need to be fetched from the Key Vault 
        id: myGetSecretAction

            # Obtain keys from the shared keyvault and store them in {steps.sharedGetSecretAction.outputs}
      - name: Azure key vault - Get Secrets from shared
        uses: Azure/get-keyvault-secrets@v1.2
        with:
          keyvault: "kvsharedresendk${{ matrix.environment.short }}"
          secrets: 'DELTA-LAKE-STORAGE-ACCOUNT-KEY,DELTA-LAKE-STORAGE-ACCOUNT-NAME,DELTA-LAKE-CONTAINER-NAME,DELTA-LAKE-EVENTS-BLOB-NAME,DELTA-LAKE-MASTER-DATA-BLOB-NAME'  # comma separated list of secret keys that need to be fetched from the Key Vault 
        id: myGetSecretAction

      - name: Set Terraform Variables  
        run: |
          echo "TF_VAR_databricks_id=${{ steps.obtain-db-id-url.outputs.workspace-id }}" >> $GITHUB_ENV 
          echo "TF_VAR_aggregation_evh_listening_key=${{steps.myGetSecretAction.outputs.aggregation-evh-listening-key}}" >> $GITHUB_ENV
          echo "TF_VAR_aggregation_storage_account_key=${{steps.myGetSecretAction.outputs.DELTA-LAKE-STORAGE-ACCOUNT-KEY}}" >> $GITHUB_ENV
          echo "TF_VAR_aggregation_storage_account_name=${{steps.myGetSecretAction.outputs.DELTA-LAKE-STORAGE-ACCOUNT-NAME}}" >> $GITHUB_ENV
          echo "TF_VAR_delta_lake_container_name=${{steps.myGetSecretAction.outputs.DELTA-LAKE-CONTAINER-NAME}}" >> $GITHUB_ENV
          echo "TF_VAR_events_data_blob_name=${{steps.myGetSecretAction.outputs.DELTA-LAKE-EVENTS-BLOB-NAME}}" >> $GITHUB_ENV
          echo "TF_VAR_master_data_blob_name=${{steps.myGetSecretAction.outputs.DELTA-LAKE-MASTER-DATA-BLOB-NAME}}" >> $GITHUB_ENV

      - name: Check If Terraform State Storage exists
        id: state-storage-exists
        run: |
          storage_exists=$(az storage account check-name --name 'tfstatedbcluster${{ matrix.environment.short }}' | python3 -c "import sys, json; print(not json.load(sys.stdin)['nameAvailable'])")
          echo "::set-output name=state-storage-exists::${storage_exists}"
      


      # Create TF State Container if needed
      - name: Create Terraform State Storage
        run: |
          storage_name="tfstatedbcluster${{ matrix.environment.short }}"
          az storage account create --resource-group ${{ matrix.environment.long }} --name $storage_name --sku Standard_LRS --encryption-services blob
          account_key=$(az storage account keys list --resource-group ${{ matrix.environment.long }} --account-name $storage_name --query '[0].value' -o tsv)
          az storage container create --name tfstate --account-name $storage_name --account-key $account_key
        if: steps.state-storage-exists.outputs.state-storage-exists == 'False'


      - name: Configure Terraform Backend
        uses: ./.github/actions/configure-terraform-backend
        with:
          backend-file-path: "./build/databricks_streaming_cluster/backend.tf"
          resource-group-name: "${{ matrix.environment.long }}"
          storage-account-name: "tfstatedbcluster${{ matrix.environment.short }}"
          

      # Create streaming job cluster
      - name: Terraform Databricks Init
        working-directory: ./build/databricks_streaming_cluster
        run: terraform init
                  
      - name: Terraform Databricks Apply
        id: terraform-apply
        working-directory: ./build/databricks_streaming_cluster
        run: |
          terraform apply -no-color -auto-approve
          echo "::set-output name=job-ids::$(terraform output databricks_job_ids)"
        continue-on-error: false
